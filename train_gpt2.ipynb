{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "w4SXPuhynJYR"
   },
   "outputs": [],
   "source": [
    "# Solving for residual std scaling issue\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ut-4Limsnq9D"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "mQJDYqd3nwu7"
   },
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANGPT_SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_70AjerYoGvR"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eUcdsWsvoK4H"
   },
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # weight sharing\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kacRu6Nkoavn",
    "outputId": "68fba69a-5d92-45e9-afab-070f22451947"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n"
     ]
    }
   ],
   "source": [
    "# model = GPT.from_pretrained('gpt2')\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "# SEED\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "# STOP\n",
    "num_return_sequences = 4\n",
    "max_length = 30\n",
    "\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # at init load tokens from disk and store them in memory\n",
    "        with open('input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        #enc = tiktoken.get_encoding('gpt2')\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        print(f'loaded {len(self.tokens)} tokens')\n",
    "        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
    "\n",
    "        # state\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B*T\n",
    "        # if loading the next batch would be out of bounds, reset\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g91UikSLog7Q",
    "outputId": "e1442476-82fd-48ed-be86-e0fe87ae63fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 338025 tokens\n",
      "1 epoch = 330 batches\n",
      "step     0 | loss: 10.972173 | lr: 6.00e-07 | norm: 8.1536 | dt: 1285.99ms | tok/sec: 3185.10\n",
      "step   100 | loss: 6.610498 | lr: 6.06e-05 | norm: 1.6423 | dt: 828.18ms | tok/sec: 4945.79\n",
      "step   200 | loss: 5.443323 | lr: 1.21e-04 | norm: 1.7812 | dt: 823.53ms | tok/sec: 4973.71\n",
      "step   300 | loss: 5.132006 | lr: 1.81e-04 | norm: 2.4597 | dt: 825.24ms | tok/sec: 4963.38\n",
      "step   400 | loss: 3.923151 | lr: 2.41e-04 | norm: 1.6863 | dt: 825.54ms | tok/sec: 4961.57\n",
      "step   500 | loss: 3.623810 | lr: 3.01e-04 | norm: 2.3819 | dt: 863.00ms | tok/sec: 4746.25\n",
      "step   600 | loss: 3.727145 | lr: 3.61e-04 | norm: 2.4957 | dt: 898.17ms | tok/sec: 4560.39\n",
      "step   700 | loss: 3.192952 | lr: 4.21e-04 | norm: 2.5304 | dt: 901.64ms | tok/sec: 4542.84\n",
      "step   800 | loss: 3.321556 | lr: 4.81e-04 | norm: 2.8482 | dt: 913.00ms | tok/sec: 4486.33\n",
      "step   900 | loss: 2.585508 | lr: 5.41e-04 | norm: 2.6439 | dt: 922.06ms | tok/sec: 4442.24\n",
      "step  1000 | loss: 2.464156 | lr: 6.00e-04 | norm: 2.9164 | dt: 929.96ms | tok/sec: 4404.49\n",
      "step  1100 | loss: 2.162601 | lr: 6.00e-04 | norm: 2.9670 | dt: 929.02ms | tok/sec: 4408.93\n",
      "step  1200 | loss: 1.539649 | lr: 6.00e-04 | norm: 2.6864 | dt: 931.08ms | tok/sec: 4399.18\n",
      "step  1300 | loss: 1.428841 | lr: 6.00e-04 | norm: 3.1717 | dt: 935.67ms | tok/sec: 4377.61\n",
      "step  1400 | loss: 1.094837 | lr: 6.00e-04 | norm: 2.8669 | dt: 954.07ms | tok/sec: 4293.19\n",
      "step  1500 | loss: 0.704205 | lr: 6.00e-04 | norm: 2.3031 | dt: 939.19ms | tok/sec: 4361.21\n",
      "step  1600 | loss: 0.557876 | lr: 6.00e-04 | norm: 2.3834 | dt: 948.54ms | tok/sec: 4318.22\n",
      "step  1700 | loss: 0.312873 | lr: 6.00e-04 | norm: 1.8705 | dt: 951.31ms | tok/sec: 4305.66\n",
      "step  1800 | loss: 0.299214 | lr: 6.00e-04 | norm: 1.8301 | dt: 950.66ms | tok/sec: 4308.59\n",
      "step  1900 | loss: 0.271631 | lr: 6.00e-04 | norm: 1.6028 | dt: 950.79ms | tok/sec: 4307.99\n",
      "step  2000 | loss: 0.199242 | lr: 6.00e-04 | norm: 1.4048 | dt: 951.12ms | tok/sec: 4306.48\n",
      "step  2100 | loss: 0.237976 | lr: 6.00e-04 | norm: 1.5320 | dt: 959.08ms | tok/sec: 4270.78\n",
      "step  2200 | loss: 0.213596 | lr: 6.00e-04 | norm: 1.4640 | dt: 954.18ms | tok/sec: 4292.69\n",
      "step  2300 | loss: 0.220805 | lr: 6.00e-04 | norm: 1.5651 | dt: 956.08ms | tok/sec: 4284.17\n",
      "step  2400 | loss: 0.196969 | lr: 6.00e-04 | norm: 1.3680 | dt: 958.48ms | tok/sec: 4273.44\n",
      "step  2500 | loss: 0.168282 | lr: 6.00e-04 | norm: 1.2138 | dt: 949.83ms | tok/sec: 4312.36\n",
      "step  2600 | loss: 0.175912 | lr: 6.00e-04 | norm: 1.2589 | dt: 955.64ms | tok/sec: 4286.15\n",
      "step  2700 | loss: 0.163991 | lr: 6.00e-04 | norm: 1.2571 | dt: 956.29ms | tok/sec: 4283.22\n",
      "step  2800 | loss: 0.163822 | lr: 6.00e-04 | norm: 1.2699 | dt: 950.51ms | tok/sec: 4309.27\n",
      "step  2900 | loss: 0.154212 | lr: 6.00e-04 | norm: 1.1951 | dt: 956.41ms | tok/sec: 4282.68\n",
      "step  3000 | loss: 0.143851 | lr: 6.00e-04 | norm: 1.1249 | dt: 956.73ms | tok/sec: 4281.25\n",
      "step  3100 | loss: 0.129326 | lr: 6.00e-04 | norm: 1.0206 | dt: 987.48ms | tok/sec: 4147.93\n",
      "step  3200 | loss: 0.136389 | lr: 6.00e-04 | norm: 1.1100 | dt: 960.72ms | tok/sec: 4263.46\n",
      "step  3300 | loss: 0.158883 | lr: 6.00e-04 | norm: 1.0945 | dt: 961.38ms | tok/sec: 4260.53\n",
      "step  3400 | loss: 0.128770 | lr: 6.00e-04 | norm: 1.0229 | dt: 956.02ms | tok/sec: 4284.42\n",
      "step  3500 | loss: 0.128802 | lr: 6.00e-04 | norm: 1.0287 | dt: 960.37ms | tok/sec: 4265.01\n",
      "\n",
      "ðŸŽ‰ Target loss achieved at step 3533!\n",
      "Final loss: 0.0890670120716095\n",
      ">  of Angelo\n",
      "Accuse him home and home. For my poor self,\n",
      "I am combined by a sacred vow\n",
      "And shall be absent. Wend\n",
      ">  Who's here?\n",
      "\n",
      "LUCIO:\n",
      "Good even. Friar, where's the provost?\n",
      "\n",
      "DUKE VINC\n",
      ">  red: thou must be patient. I am fain\n",
      "to dine and sup with water and bran; I dare not for\n",
      "my head\n",
      ">  Isabel, I loved thy brother:\n",
      "if the old fantastical duke of dark corners had been\n",
      "at home, he had lived.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "\n",
    "# IMPROVED TRAINING CONFIGURATION\n",
    "# Increase batch size and sequence length for better gradient estimates\n",
    "B = 16  # Increased from 4\n",
    "T = 64  # Increased from 32\n",
    "train_loader = DataLoaderLite(B=B, T=T)\n",
    "\n",
    "# Training hyperparameters\n",
    "max_steps = 300000  # Increased max steps\n",
    "grad_accum_steps = 4  # Gradient accumulation for effective batch size of 64\n",
    "max_lr = 6e-4  # Peak learning rate\n",
    "min_lr = max_lr * 0.1  # Minimum learning rate (10% of max)\n",
    "warmup_steps = 1000  # Learning rate warmup\n",
    "grad_clip = 1.0  # Gradient clipping value\n",
    "\n",
    "def get_lr(step):\n",
    "    \"\"\"Cosine learning rate schedule with warmup\"\"\"\n",
    "    # 1) Linear warmup for warmup_steps\n",
    "    if step < warmup_steps:\n",
    "        return max_lr * (step + 1) / warmup_steps\n",
    "    # 2) If step > max_steps, return min learning rate\n",
    "    if step > max_steps:\n",
    "        return min_lr\n",
    "    # 3) In between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff starts at 1 and goes to 0\n",
    "    return min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                              lr=max_lr,\n",
    "                              betas=(0.9, 0.95),\n",
    "                              weight_decay=0.1)\n",
    "\n",
    "# Training loop with improvements\n",
    "model.train()\n",
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "\n",
    "    # Gradient accumulation\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits, loss = model(x, y)\n",
    "        loss = loss / grad_accum_steps  # Scale loss for accumulation\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "\n",
    "    # Gradient clipping\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "    # Update learning rate\n",
    "    lr = get_lr(step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    # Wait for GPU to finish work\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    elif device == \"mps\":\n",
    "        torch.mps.synchronize()\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000  # time in milliseconds\n",
    "    tokens_per_sec = (B * T * grad_accum_steps) / (t1 - t0)\n",
    "\n",
    "    # Print progress every 100 steps\n",
    "    if step % 100 == 0:\n",
    "        print(f'step {step:5d} | loss: {loss_accum.item():.6f} | lr: {lr:.2e} | norm: {norm:.4f} | dt: {dt:.2f}ms | tok/sec: {tokens_per_sec:.2f}')\n",
    "\n",
    "    # Check if target loss is achieved\n",
    "    if loss_accum.item() < 0.09:\n",
    "        print(f'\\nðŸŽ‰ Target loss achieved at step {step}!')\n",
    "        break\n",
    "\n",
    "\n",
    "print(f'Final loss: {loss_accum}')\n",
    "#import sys; sys.exit(0)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "while x.size(1) < max_length:\n",
    "    # forward the model to get the logits\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)[0] # (B, T, vocab_size)\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        # select a token from the top-k probabilities\n",
    "        # note: multinomial does not demand the input to sum to 1\n",
    "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "        # append to the sequence\n",
    "        x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "# print the generated text\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = x[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Save the trained model for deployment\nprint(\"Saving model...\")\ntorch.save(model.state_dict(), 'gpt2_model.pt')\nprint(f\"Model saved successfully as 'gpt2_model.pt'\")\nprint(f\"Model size: {os.path.getsize('gpt2_model.pt') / (1024**2):.2f} MB\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}